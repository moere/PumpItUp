---
title: "Machine Learning II - MBD16 - Competition"
output:
  html_document: default
  html_notebook: default
---

#Table of content

1. Introduction and data preparation
2. Evaluation
3. Feature Engineering
4. Feature Selection
5. Model Selection
6. Evaluating Models

# 1. Introduction and data preparation

```{r message=FALSE, warning=FALSE}
library(randomForest)
library(caret)
library(dplyr)
```

Read and map csv files to single data frame

```{r Setup, message = FALSE, eval=FALSE}

df_labels <- read.csv("internal-training-labels.csv") # Read Labels file
df_values <- read.csv("internal-training-values.csv") # Read Values file
df <- merge(df_labels,df_values, by = "id") # Merge both files on 'ID'
rm(df_labels) # remove not used data frame
rm(df_values) # remove not used data frame

# first look at the data and it's structure
###???remove
summary(df) 
str(df)

```

It can be observed, that there are various variables that miss a significant amount of fields. Here we handle this part.

```{r}



#population '0' values

#num_private '0' values

#latitude, longitude, height: check '0' values

```

# 2. Evaluation

Build Evaluation function that computes the accuracy rate of a model

Build CV function that splits the data set into k-folds for cross validation

```{r}

splitdf <- function(dataframe, seed=NULL, percentage=0.8) {
  if (!is.null(seed)) {
    set.seed(seed)
    }
  index <- 1:nrow(dataframe)
  numTrainingSamples <- round(length(index) * percentage)
  trainindex <- sample(index, numTrainingSamples)
  trainset <- dataframe[trainindex, ]
  testset <- dataframe[-trainindex, ]
  list(trainset=trainset,testset=testset)
}

evaluation <- function(model, split){
  probs <- predict(model, type="response", newdata = split$testset)
  predictions <- data.frame(Survived = split$testset$Survived, pred=probs)
  myROC <- roc(Survived ~ probs, predictions)
  optimalThreshold <- coords(myROC, "best", ret = "threshold")
  T <- table(predictions$Survived, predictions$pred > optimalThreshold)
  F1 <- (2*(T[1,1]))/((2*(T[1,1]))+T[2,1]+T[1,2])
  F1
}

```

# 3. Feature Engineering

Build extra features
continuous to levels

https://github.com/drivendataorg/pump-it-up/blob/master/madRid/PumpItUp_DataDriven_2nd_place_madRid_team.R


```{r}

# Installer - reduce factor levels
all_WP$installer <- as.factor(all_WP$installer)
all_WP$installer[all_WP$installer == "" | all_WP$installer == 0] <- NA
all_WP$installer[all_WP$installer == "gove" | all_WP$installer == "gover" | all_WP$installer == "central government" | all_WP$installer == "central govt"] <- "government"
all_WP$installer[all_WP$installer == "commu"] <- "community"
all_WP$installer[all_WP$installer == "danid"] <- "danida"
all_WP$installer[all_WP$installer == "word" | all_WP$installer == "wo" | all_WP$installer == "word bank" | all_WP$installer == "world" | all_WP$installer == "wordl bank" | all_WP$installer == "would bank" | all_WP$installer == "world banks" | all_WP$installer == "world nk"] <- "world bank"


```

# 4. Feature Selection

Build feature selection part

```{r}

featureselection <- function(df, split){
  highmcorr <- findCorrelation(cor(df[,1:length(df)-1]), cutoff = 0.8)
  split$trainset <- split$trainset[,-highmcorr-1]
  split$testset <- split$testset[,-highmcorr-1]
}

```

# 5. Modeling Section

Build modeling or model selection part

```{r}

##For testing purposes:
df2 <- data.frame(lapply(df, function(x) as.numeric(as.character(x))))
df2 <- data.frame(lapply(df2, function(x) as.numeric((x))))
df2$status_group <- as.factor(df$status_group)


split1 <- splitdf(df2)

model1 <- randomForest(status_group ~ ., mtry = 6, data = split1$trainset, importance = TRUE, ntree = 500)

final_model <- c("Model", "Error Rate", "Classification Rate")
final_eval$model <- model1 

```

# 6. Model Evaluation and Model Selection

Store models and evaluations

```{r}



```

