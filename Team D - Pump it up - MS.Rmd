---
title: "Machine Learning II - MBD16 - Competition"
output:
  html_document: default
  html_notebook: default
---

#Table of content

1. Introduction and data preparation
2. Evaluation
3. Feature Engineering
4. Feature Selection
5. Model Selection
6. Evaluating Models

# 1. Introduction and data preparation

```{r message=FALSE}
library(randomForest)
```

Read and map csv files to single data frame

```{r Setup, message = FALSE, eval=FALSE}

df_labels <- read.csv("internal-training-labels.csv") # Read Labels file
df_values <- read.csv("internal-training-values.csv") # Read Values file
df <- merge(df_labels,df_values, by = "id") # Merge both files on 'ID'
rm(df_labels) # remove not used data frame
rm(df_values) # remove not used data frame

# first look at the data and it's structure
summary(df) 
str(df)

```

It can be observed, that there are various variables that miss a significant amount of fields. Here we handle this part.

```{r}

#population '0' values

#num_private '0' values

#latitude, longitude, height: check '0' values

```

# 2. Evaluation

Build Evaluation function that computes the accuracy rate of a model

Build CV function that splits the data set into k-folds for cross validation

```{r}

splitdf <- function(dataframe, seed=NULL, percentage=0.8) {
  if (!is.null(seed)) {
    set.seed(seed)
    }
  index <- 1:nrow(dataframe)
  numTrainingSamples <- round(length(index) * percentage)
  trainindex <- sample(index, numTrainingSamples)
  trainset <- dataframe[trainindex, ]
  testset <- dataframe[-trainindex, ]
  list(trainset=trainset,testset=testset)
}

evaluation <- function(model, split){
  model <- tree(status_group~., data = split$trainset, family = "binomial")
  probs <- predict(model, type="response", newdata = split$testset)
  predictions <- data.frame(Survived = split$testset$Survived, pred=probs)
  myROC <- roc(Survived ~ probs, predictions)
  optimalThreshold <- coords(myROC, "best", ret = "threshold")
  T <- table(predictions$Survived, predictions$pred > optimalThreshold)
  F1 <- (2*(T[1,1]))/((2*(T[1,1]))+T[2,1]+T[1,2])
  F1
}

```

# 3. Feature Engineering

Build extra features
continuous to levels

https://github.com/drivendataorg/pump-it-up/blob/master/madRid/PumpItUp_DataDriven_2nd_place_madRid_team.R


```{r}

```

# 4. Feature Selection

Build feature selection part

```{r}

featureselection <- function(df, split){
  highmcorr <- findCorrelation(cor(df[,1:length(df)-1]), cutoff = 0.8)
  split$trainset <- split$trainset[,-highmcorr-1]
  split$testset <- split$testset[,-highmcorr-1]
}

```

# 5. Modeling Section

Build modeling or model selection part

```{r}
df1 <- lapply(df, ifelse(is.integer()))
model1 <- randomForest(status_group ~ ., data = df, importance = TRUE, proximity = TRUE)

```

# 6. Model Evaluation and Model Selection

Store models and evaluations

```{r}



```

